---
title: "SOSC5500_Assignment1_Group14"
author: "Mar 07, 2022"
date: "2022/3/7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

### Import & format the datasets ###
```{r}
train_and_val<-read.csv("census-income-training.csv")
test<-read.csv("census-income-test.csv")

# inspect the datasets
dim(train_and_val)
dim(test)

# We noticed the variable "PARENT" only has one level, we remove this variable from all datasets
train_and_val<-train_and_val[, -31]
test<-test[, -31]

# Let's extract 10% test & val data to be the validation data, rest are the training data
val.num<-round(dim(train_and_val)[1]*0.2)
val.index<-sample(nrow(train_and_val), val.num, replace = F)
val<-train_and_val[val.index, ]
train<-train_and_val[-val.index, ]

```

### Lasso regression ###
```{r}
# Import package for Ridge and Lasso
library(glmnet)
```

```{r}
# extract matrix data from train dataset
train_data<-model.matrix(~.-1, train[, -40:-41])
train_y<-train$income_morethan_50K
dim(train_data)

# by default, alpha=1, i.e., lasso regression, nlambda=100
fit1<-glmnet(train_data, train_y, family="binomial", nlambda=20)
plot(fit1, xvar = "lambda", label = TRUE)
print(fit1)
# From the result we can see that when lambda is bigger than 0.003, the decrease of %Dev becomes more rapid.  
```

```{r}
### functions for accuracy, precision and recall
# accuracy function
accuracy<-function(ypred, y){
        tab<-table(ypred, y)
        return(sum(diag(tab))/sum(tab))
}
# precision
precision <- function(ypred, y){
    tab <- table(ypred, y)
    return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# recall
recall <- function(ypred, y){
    tab <- table(ypred, y)
    return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# F1 score
F1 <- function(ypred, y){
  prec <- precision (ypred, y)
  rec <- recall (ypred, y)
  return ( ( 2 * prec * rec) / (prec + rec))
}
```


```{r}
# make predictions using the valication dataset, based on the lasso model above
# prepare val dataset
val_data<-model.matrix(~.-1, val[, -40:-41])
val_y<-val$income_morethan_50K

# loop the lambda scores to find the one with least F1
# from the fit1 output, we choose the first lambda list as below
lambda_list1<-seq(0.0001, 0.001, 0.0001)

# lasso regressions
F1_list<-data.frame()
accuracy_list<-data.frame()
precision_list<-data.frame()
recall_list<-data.frame()
for (i in 1:10){
        #i<-1
        print(i)
        lambda_n<-lambda_list1[i]
        fitn<-glmnet(train_data, train_y, family="binomial",
                     lambda = lambda_n, alpha = 1)
        pred_y<-predict(fitn, newx=val_data, type="class")
        head(pred_y)
        F1_list<-c(F1_list, F1(pred_y, val_y))
        accuracy_list<-c(accuracy_list, accuracy(pred_y, val_y))
        precision_list<-c(precision_list, precision(pred_y, val_y))
        recall_list<-c(recall_list, recall(pred_y, val_y))
}
F1_list1<-F1_list
accuracy_list1<-accuracy_list
precision_list1<-precision_list
recall_list1<-recall_list
# The summary table of performance
T1<-cbind(lambda_list1, F1_list1, accuracy_list1, 
          precision_list1, recall_list1)

# The F1 score decreased with the increased lambda values, we try another ineration loop, make the lambda smaller 
lambda_list2<-seq(0.00001, 0.0001, 0.00001)

# lasso regressions
F1_list<-data.frame()
accuracy_list<-data.frame()
precision_list<-data.frame()
recall_list<-data.frame()
for (i in 1:10){
        #i<-1
        print(i)
        lambda_n<-lambda_list2[i]
        fitn<-glmnet(train_data, train_y, family="binomial",
                     lambda = lambda_n, alpha = 1)
        pred_y<-predict(fitn, newx=val_data, type="class")
        head(pred_y)
        F1_list<-c(F1_list, F1(pred_y, val_y))
        accuracy_list<-c(accuracy_list, accuracy(pred_y, val_y))
        precision_list<-c(precision_list, precision(pred_y, val_y))
        recall_list<-c(recall_list, recall(pred_y, val_y))
}
F1_list2<-F1_list
accuracy_list2<-accuracy_list
precision_list2<-precision_list
recall_list2<-recall_list
# The summary table of performance
T2<-cbind(lambda_list2, F1_list2, accuracy_list2, 
          precision_list2, recall_list2)
rbind(T2, T1)
# We didn't observe a turning point of the F1 score, but we think this F1 score is good enough. Here we take the optimized lambda is 0.00001
```

## Predict outcome using the test dataset##
```{r}
# Now we set lambda as 0.00001 for a biggest F1 score. 
# The lasso regression model is
fit<-glmnet(train_data, train_y, family="binomial",
                     lambda = 0.00001, alpha = 1)
test_data<-model.matrix(~.-1, test[, -40])
test_pred_y<-predict(fit, newx=test_data, type="class")
```

```{r}
# rename outcome variable
income_morethan_50K<-as.data.frame(test_pred_y)
income_morethan_50K$Id<-test$Id
colnames(income_morethan_50K)<-c("income_morethan_50K", "Id")
write.csv(income_morethan_50K,
          file="G14_attempt1_income_morethan_50K.csv", row.names = F)
```







